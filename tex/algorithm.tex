\documentclass[main.tex]{subfiles}
\begin{document}

	
	In the followings, we will discuss the methodology of the extension eliminating process. The phases will be introduced in their respective order, and their functionalities will be demonstrated using concrete examples. An overview of the whole algorithm can be seen on Figure~\ref{fig:algo-proc}. The nodes in the diagram illustrate the information different phases of the algorithm work with; the edges between the nodes represent the phases themselves. On the figure, possible configuration points of the algorithm are marked in italics. Also, certain stages have additional information associated with them, they are indicated by the gray labels.	
	
	\begin{figure}[t] 
		\centering
		\subfile{algorithm_process_diag}
	\end{figure}
	
	\subsection{Individual checkers}
	
	In the first phase of the algorithm, the individual extension checkers statically analyze the type-annotated syntax tree provided by Haskell-Tools. These checkers calculate so-called \emph{extension formulas} for the nodes in the syntax tree. Each checker only analyzes those type of nodes, that can potentially require the corresponding language extension. For example, the \chk{MagicHashChecker}, which allows the \texttt{\#} character as a postfix modifier on identifiers, literals and kinds, needs to inspect those respective types of nodes. These checking mechanics can be implemented independently of each other. Essentially, we define certain node types that the extension checker will analyze during the traversal of the syntax tree. When a certain type of node is encountered, the algorithm will run all corresponding checks of all extension checkers. Using a general traversal method, the algorithm only has to visit each node a single time, traversing the whole tree just once. The other three components will be discussed in detail below.
	
	%NOTE: THIS IS REALLY IMPORTANT!!!
	As we will, the compiler-specific individual extension checking is very well separated from the more general phases of the algorithm. This means, that our approach of constructing and solving extension formulas can be adopted to any other compiler, so long as the individual checkers are defined. Moreover, if any new extensions are introduced by GHC, they can easily fit into this general model. We only have to define a checker for it, and the relations it has with other existing extensions.	

	\subsection{Logical formulae of extensions}
	
	Sometimes it is difficult to determine the exact set of extensions required by a language element. For instance the language element may need a feature which can be enabled by two or more different extensions. In this situation adding any one of these extensions to the module would be sufficient, but the choice would be arbitrary, and the algorithm might end up yielding a larger than minimal extension set, as illustrated by Program~code~\ref{code:ext-ambiguity-parsec}.
	
	Extensions may depend on each other in different ways. They might mutually exclude each other, or certain extensions may require turning on other ones~etc.	We can observe that the connections among various compiler extensions can be very well described using propositional logic. As a general solution, instead of associating a node with a clearly defined set of extensions, we assign a logical formula to the language element in question. The formula can contain atomic variables, which correspond to extensions; and logical operators representing the relations between the extensions. The resulting system of formulas will be processed using a SAT-solver as shown in Section~\ref{sat-solver}
	
	Using this approach, we can easily resolve the problem depicted earlier in Program~code~\ref{code:ext-ambiguity-parsec}. The ambiguity between \ext{TypeFamilies} and \ext{GADTs} induced by the type equality can be represented by the logical disjunction of the two extensions. This means, we will associate the corresponding syntax tree node with the logical formula: $\lvar{TypeFamilies} \lor \lvar{GADTs}$ \footnote{The typesetting of variables deliberately differs from that of extensions, making it clear which one is referred.}. As for the associated type \ilcode{Elem}, it can only be used by enabling \ext{TypeFamilies}, thus the formula for that node will contain just a single variable: \lvar{TypeFamilies}.
	
	\subsection{Levels of certainty} \label{certainty-levels}
	
	When analyzing a node in the syntax tree, our algorithm does not only compute the extension formula, but it also assigns a certainty level to it. The certainty level indicates how much confidence we have in the correctness of the formula associated with the node. Currently the algorithm distinguishes three levels of certainty, namely \emph{Evidence}, \emph{MissingInformation} and \emph{Hint}. 
	
	A formula is marked as \emph{Evidence} if all required information is present in the node for calculating it. This is the most frequently used certainty level, because most of the time we can exactly determine the extension formula induced by a node. In all the examples in this section every formula is considered as \emph{Evidence}, if not explicitly stated otherwise.
	
	\emph{MissingInformation} is used to explicitly indicate if the information present in the node is partial due to internal reasons. A typical use case for it is when some semantic information is not available during the analysis, because a type or name lookup failed. In such cases, to avoid false positive results, the checkers responsible for analyzing the corresponding piece of code, may still assign certain extension formulas to the given syntax tree node. They will also annotate these formulas with \emph{MissingInformation}, so that in the later phases, the algorithm can choose to either ignore these formulas, or consider them when calculating the final solution.
	
	The certainty level \emph{Hint} is similar to \emph{MissingInformation} in the sense that it is used to distinguish possibly false negative extension formulas. However, in the case of \emph{Hints}, we are not missing any, otherwise available information (due to internal failure), but we are lacking the type information of subexpressions. Due to partially available type information, we cannot always determine the extension formulas with complete accuracy. Program code~\ref{code:context-sensitivity} illustrates this problem through the \ext{FlexibleContexts} language extension. 
		
	\noindent
	\begin{codeFloat}[H]
		\begin{minipage}{0.47\textwidth}
			\begin{haskell}
				{-# LANGUAGE FlexibleContexts #-}
				module A where
				
				class C a where
				
				f :: C [a] => a -> a
				f = id
			\end{haskell}
		\end{minipage}
		\hfill
		\begin{minipage}{0.50\textwidth}
			\begin{haskell}	
				{-# LANGUAGE FlexibleInstances #-}
				module B where		
				import A
				
				instance C [Char]
				
				g x = f 'c'
			\end{haskell}
		\end{minipage}
	\caption{Example for context sensitivity}
	\label{code:context-sensitivity}
	\end{codeFloat}
	
	\noindent
	As we can see, in \ilcode{module A}, \ilcode{f} has type which requires turning on the extension. However, in \ilcode{module B}, \ilcode{f} is applied to an argument, which in turn specializes the type variable in the function's type\footnote{\ext{FlexibleInstances} is needed in order to instantiate \ilcode{class C} for \ilcode{[Char]}.}. The expression \ilcode{f 'c'} no longer has a type constraint, so it does not require \ext{FlexibleContexts}. To recognize this, we would have to analyze the expression as a whole, just like compiler does, but because type information for expressions is not available, we can only inspect the function by itself. Unfortunately, this can easily result in false negative results.
	
	As an alternative solution, the algorithm will treat such ambiguous AAST nodes differently. They will be marked as \emph{Hints}. These nodes not necessarily require any extensions in their current state, but minor changes to the source code can put those nodes into a context where they actually require the language extension. As an example, changing the argument of \ilcode{f} in Program~code~\ref{code:context-sensitivity} from the character \ilcode{'c'} to the variable \ilcode{x} will make \ext{FlexibleContexts} required. By showing hints to the user, we can warn them to treat similar language elements with special care.
	
	After all extension formulas and their certainty levels are evaluated, the algorithm decides which formulas it should consider for calculating the minimal extension set. These decisions are based on certain policies defined by the user, who can specify how the algorithm should treat each certainty level. For example, the user can choose to ignore hints, in which case the algorithm will compute the final minimal set using only the formulas of the other two certainty levels. This approach makes it possible to handle exceptional situations in a broader context using \emph{MissingInformation}, and to configure the aggressiveness of the extension removal using \emph{Hints}.
	
	\subsection{Solving the formulas} \label{sat-solver}
	
	After calculating the extension formulas, and filtering them based on certainty levels; the next step is to determine the minimal set of extensions the module requires. This can be achieved by solving the conjunction of the previously calculated formulas as a boolean satisfiability problem. In the following sections, we will refer to the extension variables in the satisfying interpretations mapped to the logical value \emph{true} as the solutions of the formula. We will demonstrate how the solutions are computed on the following example.

	\begin{codeFloat}[H]
		\begin{haskell}
			class Collection c where
			  type Elem c
			  elements :: c -> [Elem c]
		\end{haskell}
		\begin{haskell}
			data %\colorbox{lightred}{Avg (a :: *)}% where
			   %\colorbox{lightyellow}{Avg :: Int -> a -> Avg a}%
			
			genericAvg :: (Collection c,  %\colorbox{lightblue}{Elem c ~ a}%, Num a) => c -> Avg a
			genericAvg c = Avg (length es) (sum es)
			  where es = elements c
		\end{haskell}
		\caption{Formula solving example}
		\label{code:sat-solving}
	\end{codeFloat}
	
	The above piece of code calculates the generic average of the elements in a collection using the \ilcode{Collection} class defined in another module\footnote{For the sake of brevity, the import declarations are omitted.}. The first thing we notice, is that in the declaration of \ilcode{Avg}, the type variable \pilcode{a} is kind-annotated (highlighted in red). This feature is enabled by \ext{KindSignatures}. Next, \ilcode{Avg} is declared using \emph{generalized algebraic data type syntax} (highlighted in yellow), which requires the corresponding language extension, \ext{GADTSyntax}. Lastly, there is a type equality in the type signature of \ilcode{genericAvg}, which can be enabled by both \ext{GADTs} and \ext{TypeFamilies}. The resulting formula system can be seen below.
	
	\begin{align*}
		\Big\{ \{\lvar{KindSignatures}\}, \{\lvar{GADTSyntax}\}, \{\lvar{GADTs}~\lor~\lvar{TypeFamilies}\} \Big\}
	\end{align*}
	
	In order to satisfy all elements of the system, \lvar{KindSignatures} and \lvar{GADTSyntax} must be \emph{true}. As for the last formula, we have three options. We can enable either two of the extensions, or both of them. This means, the system has three distinct solutions in total.
	
	\begin{gather}
		\Big\{ \lvar{KindSignatures}, \lvar{GADTSyntax}, \lvar{GADTs} \Big\} \label{eq:set-GADTs} \\
		\Big\{ \lvar{KindSignatures}, \lvar{GADTSyntax}, \lvar{TypeFamilies} \Big\} \label{eq:set-TypeFamilies} \\
		\Big\{ \lvar{KindSignatures}, \lvar{GADTSyntax}, \lvar{GADTs}, \lvar{TypeFamilies} \Big\} \label{eq:set-both}
	\end{gather}
	
	Our approach is designed in a way to handle any system of extensions, regardless of compiler. The algorithm defined here can effectively deal with any relations of extensions, as long as the resulting formulas are solvable. In practice, constructing an extension set for a module is always possible, since the program has been successfully compiled. However, theoretically, extensions can form a very complicated network of relations, in which case, it is also entirely possible to generate an unsolvable formula system. % if the individual checkers calculate contradicting extension formulas.
	
	Fortunately, the current extensions supported by GHC have a simple implication network, that can be represented by a group of directed acyclic graphs, as discussed in Section~\ref{ext-elim_problem}. Furthermore, there are no exclusion relations\footnote{An exclusion relation between two extensions means, that turning on either of the two requires turning off the other.} among the extensions we investigate; which means, we can also avoid negating variables. From here, it can be shown, that any formula associated with a syntax tree node will contain only disjunctions and conjunctions of extension variables, hence it will always have at least one solution. Using the SAT-solver, we calculate all possible solutions to the given formula system.
	
	\subsection{Ranking the possible solutions} \label{ranking-solutions}
	
	The last phase of the algorithm consists of two smaller subphases. Since they are closely related and work with the same type of data, namely sets of extensions, they will be discussed together.	The goal of these phases is to find a minimal extension set among all the possible solutions. The first step is to minimize each solution by removing all implied extensions. As an illustrating example, we will minimize the extension sets (\ref{eq:set-GADTs})--(\ref{eq:set-both}) calculated from Program~code~\ref{code:sat-solving}. The reductions are performed using the implication relations of the extension as shown in Figure~\ref{fig:explicit-namespaces-dag}. The reduced sets can be seen below in their respective order.
	
	\begin{gather}
		\Big\{ \lvar{KindSignatures}, \lvar{GADTs} \Big\} \label{eq:reduced-set-GADTs} \\
		\Big\{ \lvar{GADTSyntax}, \lvar{TypeFamilies} \Big\} \label{eq:reduced-set-TypeFamilies} \\
		\Big\{ \lvar{GADTs}, \lvar{TypeFamilies} \Big\} \label{eq:reduced-set-both}
	\end{gather}
	
	As the final step, the algorithm ranks the minimal extension sets based on a \emph{complexity function}, which assigns a positive real number to every extension. The complexity of a set is defined as the sum of complexities of the contained extensions. The complexity function can be any positive real-valued function, but as its name suggests it should describe how complex a given extension is. A trivial implementation could be any positive constant function. In this case the algorithm would find the set with the least number of extensions. A more accurate metric can be the number of extensions the argument of the function implies. Using the second variant we can more precisely approximate how much functionality a certain extension provides.
	
	Based on the trivial implementation, the algorithm could choose any one of the sets~(\ref{eq:reduced-set-GADTs})--(\ref{eq:reduced-set-both}), since all of them contain the same number of extensions. However, using the second variant of the complexity function, while the first two sets would still have same complexities (4), the third one would have a higher complexity of 6. From these values, the algorithm can only select either set~(\ref{eq:reduced-set-GADTs}) or (\ref{eq:reduced-set-TypeFamilies}) as the final solution. Either of the first two sets are strictly better choices than the last one, since they contain simpler extensions. By using a more precise complexity function, we were able to find a more sophisticated solution.
	
	The complexity function also makes the algorithm configurable from outside. For instance, if the user would like to port their codebase to a different compiler, or to a different version of GHC; they could assign low values for extensions already supported by the target platform, so the algorithm would prefer those extensions over other ones.
		
\end{document}